{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vascomeerman/Coding/Harvard/CS215/Project/realworld-stylegan2-encoder\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Path to the psp repo in our shared folder (I believe)\n",
    "psp_path = \"v3_psp\"\n",
    "\n",
    "# Append the directory to our python path\n",
    "sys.path.append(psp_path)\n",
    "# Set current working directory\n",
    "os.chdir(psp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pprint\n",
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# sys.path.append(\".\")\n",
    "# sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "# from datasets import augmentations\n",
    "from utils.common import tensor2im, log_input_image\n",
    "from models.psp import pSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device -> cpu\n"
     ]
    }
   ],
   "source": [
    "# # I am not using a GPU here, if you are, move it to cuda\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\" # UNCOMMENT FOR CUDA \n",
    "print(f\"Running on device -> {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_type = 'ffhq_encode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_DATA_ARGS = {\n",
    "    \"ffhq_encode\": {\n",
    "        \"model_path\": \"pretrained_models/psp_ffhq_encode.pt\",\n",
    "        \"image_path\": \"notebooks/images/input_img.jpg\",\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    },\n",
    "}\n",
    "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of opset versions\n",
    "\n",
    "Versions that DON'T work (cpu)\n",
    "- 9\n",
    "- 10\n",
    "- 13\n",
    "- 14\n",
    "\n",
    "\n",
    "Version that do work withouth attenfallback (cpu)\n",
    "- 11\n",
    "- 12\n",
    "\n",
    "\n",
    "Versions only work with attentfallback (cpu):\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = EXPERIMENT_ARGS['model_path']\n",
    "OPSET_VERSION = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of shape 1000, confidence scores for each of the imagenet classes\n",
    "# Now we will save this model.\n",
    "import torch.onnx\n",
    "\n",
    "# # I am not using a GPU here, if you are, move it to cuda\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "\n",
    "def export_onnx_model(model, onnx_model_path):\n",
    "    with torch.no_grad():\n",
    "        # Bring full model to cpu for conversion\n",
    "        # model.to(device)\n",
    "\n",
    "        # Format input image into a batch format the model can use\n",
    "        # inp_batch = transformed_image1.unsqueeze(0)\n",
    "        inp_batch = torch.randn(1, 3, 256, 256).to(device)\n",
    "\n",
    "\n",
    "        # Run the model once on the same input we give it for onnx to run\n",
    "        # _, _ = model(img_tensor.float(), randomize_noise=False, return_latents= True)\n",
    "        print(\"\\n\\n --------------ONNXX LETS GO ------------------\\n\\n\")\n",
    "\n",
    "        # Inputs needed for onnx to run 1 inference session so it can determine the graph it needs to convert\n",
    "        inputs = (inp_batch.float(),\n",
    "                  {\"randomize_noise\": False,\n",
    "                  \"return_latents\": True})\n",
    "        \n",
    "        # Convert our pytroch model to onnx, so we can later on convert it to tf\n",
    "        torch.onnx.export(model,                                            \n",
    "                          inputs,\n",
    "                          onnx_model_path,                                  \n",
    "                          opset_version=OPSET_VERSION,\n",
    "                          do_constant_folding=True,\n",
    "                          # See: https://github.com/pytorch/fairseq/issues/3395\n",
    "                          operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK,\n",
    "                          export_params=True,\n",
    "                        #   input_names=['input'],\n",
    "                        #   output_names=['output'],               \n",
    "                          # input_names=['input_ids',                         \n",
    "                          #              'input_code',\n",
    "                          #              'return_latents']\n",
    "                          )\n",
    "        \n",
    "        print(\"ONNX Model exported to {0}\".format(onnx_model_path))\n",
    "\n",
    "\n",
    "\n",
    "def export_new_model():\n",
    "    ckpt2 = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "    # update the training options\n",
    "    opts2 = ckpt2['opts']\n",
    "    if device == \"cpu\":\n",
    "        opts2['device'] = \"cpu\"\n",
    "\n",
    "    opts2['checkpoint_path'] = model_path\n",
    "    if 'learn_in_w' not in opts2:\n",
    "        opts2['learn_in_w'] = False\n",
    "    if 'output_size' not in opts2:\n",
    "        opts2['output_size'] = 1024\n",
    "\n",
    "\n",
    "    opts2 = Namespace(**opts2)\n",
    "    new_net = pSp(opts2)\n",
    "    new_net.eval()\n",
    "\n",
    "    if device != \"cpu\":\n",
    "        new_net.cuda() # Don't move model to cuda for conversion\n",
    "    else:\n",
    "        print(f\"No cuda -> Running on {device}\")\n",
    "\n",
    "    export_onnx_model(new_net, f\"pretrained_models/psp_clean_ops{OPSET_VERSION}.onnx\")\n",
    "\n",
    "    \n",
    "# export_new_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the ONNX model to TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_MODEL_CPU = \"psp_clean_\"\n",
    "WEIGHT_PATH = 'pretrained_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "tic = time.time()\n",
    "\n",
    "# Load ONNX model and convert to TensorFlow format\n",
    "model_psp_onnx = onnx.load(f'{CURRENT_MODEL_CPU}{CURRENT_MODEL_CPU}.onnx')\n",
    "# model_decoder_onnx = onnx.load('../output/decoder.onnx')\n",
    "\n",
    "# Load onnx model into onnx_tf representation\n",
    "tf_psp_rep = prepare(model_psp_onnx, device=\"CPU\")\n",
    "# tf_decoder_rep = prepare(model_decoder_onnx)\n",
    "\n",
    "# Timer ends\n",
    "toc = time.time()\n",
    "print('Loading onnx models to tf, took {:.4f} seconds.'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77ed5795d16ccc9261959c2cdd3d7136049904981f0ebcd029416055b8f109ae"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('MLOpsProject': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
